## 概述
分布式存储系统的特点：**可扩展，低成本，高性能，易用**
主要技术：**数据分布，一致性，容错，负载均衡，事务与并发控制，易用性设计，压缩／解压缩**
1. 分布式文件系统
存储数据类型：**Blob（Binary Large Object），定长块，大文件**
2. 分布式健值系统
3. 分布式表格系统
4. 分布式数据库

## 单机存储
Hash表，B树在机械磁盘，SSD的实现就是单机存储引擎。
NUMA架构，每个节点是一个SMP结构，以网络链接
为避免网络拓扑结构的依赖，Google采用扁平拓扑结构，三级CLOS网络
先知道硬件的性能指标和价格，才能知道瓶颈，要用什么技术，解决那里的问题，比如硬盘的随机读写慢问题
SSD有写入放大问题（[Write amplification](https://zh.wikipedia.org/wiki/写入放大))，会影响其实际写入带宽

### Hash存储引擎
1. Bitcask 仅支持追加操作，文件写到固定大小后开新的文件，同一时刻只有一个新的文件，hash索引每个key对应的文件id，长度，偏移，以便于找到对应的value，对于删除，只修改value值为特殊值，不真的删除
2. 定期合并删除和被更新后的垃圾数据，生成新文件
3. 快速恢复，防止内存中的hash索引丢失，每次合并删除时，创建索引文件，相当于把内存中的索引dump到硬盘，恢复时直接读文件

### B树存储引擎
关系数据库以此为底层存储实现，支持随机存储和范围扫描，如InnoDb
数据库的缓存算法LIRS，把缓存分为两级，热点数据进入第二级，第一季采用LRU替换，防止全表扫描等操作使得缓存池中的大量页面被换出

### LSM树存储引擎
log Structured Merge Tree，把数据的修改增量缓存在内存中，批量操作磁盘。读取时合并磁盘和内存数据。
LevelDB，数据写入时，先写日志，然后写内存，内存满了，开一块新的继续写，写的操作不能被hold住。硬盘存的文件有顺序性，和一些索引信息，使得内存数据写入硬盘时性能好

### 数据模型
* 文件模型，对象模式弱化的文件模型，POSIX标准
* 关系模型，表格，SQL
* 键值模型，NoSql，put get delete

传统SQL数据库在海量数据场景面临的挑战：
1. 事务，ACID特性
2. 联表，违背第三范式，A表中存B表中的非主键信息
3. 性能

### 事务与并发控制
COW(Copy on Write) & MVCC(Muti-Version Concurrency Control)
可串行化：多个事务并发执行的结果，和按照某个顺序串行执行的结果相同
MVCC:以InnoDB为例，每一行存一个被修改的“时间”和删除的“时间”，这里的时间就是数据库的版本号，每次执行一个事务时，会在分配一个版本号给该事务，根据此事务号和每行的版本号比较，判断某一行是否该参与此事务。Update时，会先copy一份
故障恢复时纪录的日志，UNDO／REDO 顾名思义，简化日志的记法，批量刷日志到磁盘。定期dump数据，checkpoint技术。REDO日志纪录的都是修改后的结果，使得回放操作具有幂等性

### 压缩
LZ算法，后续的压缩都在此体系下，根据效率和压缩比例改进调整，以列存储实现OLAP数据库，提高压缩比例，减少查询时间

## 分布式系统
木桶原理 －> 发现短板 －> 性能估算
Paxos协议&两段提交协议
分布式系统的三态：成功，失败，超时，对于超时，要么操作幂等反复重试，要么获取刚才的操作状态，以此判断
越是强的一致性模型，使用越简单

### 性能分析
排序性能分析方法：排序时间＝比较时间（分支预测错误，5ns左右）＋ 内存访问时间（4GB/s 内存访问速度）
1GB 4字节整数快排约28s

### 数据分布
#### Hash分布
* 对于大用户处理方式：
1. 手动拆分，定时跑任务，配置拆分规则
2. 自动拆分，由数据分布算法动态调整
* 机器变化时Hash的N值变化，数据要重新部署，解决：
1. 把hash和机器的映射元信息由专门的服务器管理而不是简单的模运算
2. 采用一致性Hash distributed hash table，给每个节点分配随机token，这些token构成hash环。先计算key的hash，然后放到顺时针第一个大于或等于此hash值的token所在的节点。有点，节点删除或增加时，只影响hash环中相邻节点。但还要考虑迁移执行时负载不均衡。每台节点都纪录所有的hash环的信息。
#### 顺序分布
需要考虑顺序数据表的分裂和合并

### 负载均衡
是个30-60分钟的慢过程，新节点加入时，master不可大量迁移数据到新机器

### 复制
强同步复制，异步复制
同步常用的做法是同步操作日志，副本上回放日志
除了主副本＋复制协议，还有写多个节点的复制协议，不分主副，一共N个，W个写副本，R个读副本，保证W＋R>N
CAP理论
Oracle的最大可用模式：正常时强同步复制，主备网络故障时，切换为异步复制

### 容错
租约Lease协议，原因，异步网络中的多台机器无法达成一致。
A机器给B机器发送带有时间的租约，时间会有时间服务器同步

### 扩展性
水平扩展除了考虑master节点的瓶颈，也需要考虑故障恢复和自动化扩容的灵活性
master瓶颈时，增加一层，来存元数据，由于客户机会缓存中间的元数据服务器的信息，所以不会增加额外网络消耗
扩容，双倍扩容的劣势
应采用异构系统，而非同构系统，同构系统需要其中一个分片copy扩容，慢，异构时数据分散在所有系统，由全系统来copy数据扩容

### 分机房
多个副本构成一个复制组，靠Paxos协议保证只有一个主副本，不需要master节点发放租约，master故障不影响worker，工程复杂度高

## 分布式文件系统
一方面存储Blob文件，一方面是分布式数据库的底层存储
### GFS
分为master，chunkserver和client
1. 租约机制
为了防止所有的写操作都通过master，通过lease给某一个chunkserver，授权为主chunkserver，可以执行写操作。每个chunk维护版本号，解决不一直问题，每次发租约副本的版本号加1，版本号低的chunk进行定是垃圾回收
2. 一致性模型
GFS is designed for append not for overwrite, append is simple。需要解决追加执行多次，产生重复。多个客户端追加的顺序性，GFS不保证一个client的多次追加是连续的
3. 追加流程
请求master获取所有chunk元信息，数据client发给每一个副本，每个chunkserver内以LRU结构缓存数据，所有的副本都确认受到数据后，client发起写请求给主副本，主副本确定对同一个chunk多次append的顺序并写入，主副本把写请求发给其他副本，其他副本根据此顺序写，其他副本写完后回复主副本，主副本再应答client，如果部分从副本不成功，则client重试。
**利用流水线操作，减少延时**，当一个chunkserver收到一些数据，就立刻转发。采用全双工网络，发送不降低接受的速率。
**控制流和数据流进行分离**，利用网络拓扑更好的传递数据，每个机器把数据，发给拓扑结构上最近的另一台机器上。前提是每次追加的数据都比较大。
追加流程还需要考虑，过程中，主副本租约过期，失去chunk修改操作的授权，主副本和备副本所在的chunkserver出现故障等各种异常。
*可以从人角度考虑，租约过期就给予即两次续约申请的机会，在这个阶段hold资源，若成功则继续，否则算作失败，走失败逻辑，对于各种可能的状况，都去一一设计对应的逻辑解决可能复杂度太大，可以统一都回退到一个失败状态，然后再去处理*

4. 容错机制
master:操作日志＋checkpoint＋实时热备
chunkserver:复制多个副本容错，全部副本写入成功才认为成功，64mb的chunk中分为64kb的Block，每个block中存32位校验和。

#### Master设计
1. 需要估算维护元数据的内存占用，结论1pb数据不会超过3gb内存不会是瓶颈，采用写时复制的B树，满足元数据管理，实现复杂
2. 负载均衡，考虑网络拓扑，机器的分布，磁盘的利用率。新副本所在的chunkserver的磁盘利用率低于平均，但是要限制每个chunkserver“最近”创建的数量，防止新加一个chunkserver时瞬间的大量迁移把他压垮，所有的副本不能再同一个机架。
当chunk数目少的时候需要复制，复制的任务也有优先级，副本最少的chunk先复制。只有一个副本的chunk要禁止其写入。
3. 垃圾回收，延迟删除，先标记，后删除。chunkserver和master的心跳消息中同步master中已经不存在的chunk消息，然后chunkserver去释放副本，在使用的低峰时段做这件事。
4. 快照，写时复制生成快照，chunk利用增加引用技术的形式，master生成新的快照文件后，还指向原来的chunk。真正有写入时，再copy这个chunk，写入到新的chunk中

#### ChunkServer设计
chunk要尽量均匀分布再不同的磁盘，还要考虑磁盘空间，最近新建chunk数量，需要删除的chunk只是移动到磁盘的回收站，下次重用，作为磁盘和网络IO密集型应用，做磁盘和网络操作的异步化，最大化的发挥性能。

### Taobao File system
数据量巨大，Metadata单机无法存储
减少图片读取的IO次数，普通linux读一个文件要3此磁盘IO，读目录元数据，把文件inode节点转入内存，最后读文件
多个逻辑图片文件共享一个物理文件

#### 架构
不维护文件目录树，每个文件用64位编号，*退化为了KV的存储了就*，将大量小文件合并成一个大的块，以block id＋offset来确定文件，三备份存储，客户端不缓存文件数据
1. 追加流程
由于TFS读多于写，所以写流程简化，所有的写操作都经过master，不需要支持多个client并发追加，同一时刻每个block只能有一个写操作，多个client的写会被串行化。client从master找可写的block，客户端先写主副本，主副本再同步数据到备份副本，成功后master需要根据主副本的反馈更新block的版本号，然后再返回client成功，其中的任何失败都从第一个步开始重试
2. 其他问题
文件重复，根据文件的指纹，建立去重系统，这个网盘也是类似，去重本质上又是一个KV的存储
文件更新，写入新文件，再应用系统中保存新文件的位置
文件删除，应用系统中删除，TFS中本身不删除

### Facebook Haystack
估算每天的量时，一台计算位40000s，而没有算作86400s。
#### 架构
多个逻辑文件，共享一个物理文件
包括：目录(master)，存储，缓存。缓存用于解决对CDN供应商过于依赖的问题。
1. 写流程
通过目录获取可写的逻辑卷轴，生成id，备份3份，照片再每个物理卷轴的offset可能不同。只支持追加，以offset更大的照片为同一个逻辑卷轴中的最近照片
2. 容错
存储节点出错，将物理卷轴对应的逻辑卷轴标记为只读，不可恢复时将执行拷贝人数
目录容错，主备数据库做持久化，由数据库提供容错机制
3. 目录
根据图片id映射到对应的逻辑卷轴和其位置，估计使用的memcache或mysql sharding，因为映射的关系太大。也需要提供负载均衡
4. 存储
物理卷轴对应一个文件，存的图片除了二进制本身，还有元信息（构成一个needle），每个物理卷轴还维护一个索引文件，便于查找。先写文件，再异步更新索引文件，追加式的，更新也方便。
删除时，延迟删除，追加一个带有删除标记的needle，定时任务回收删除空间，扫描全部文件，只保留最新的，重新组织文件。
#### 其他
可以进行删除，TFS不能，TFS只存了blcokid＋offset，一旦删除了其他的，这个值就要该了，而应用系统中已经存好了，不可能改。而haystack中元信息只能定位到逻辑卷轴，需要在逻辑卷轴中再次定位才能找到文件，相当于多了一层，就可以删了。

### 内容分发网络CDN
CDN本身由两层cache，图片服务器本身还有cache，都不中才会访问分布式存储
负载均衡：LVS（4层负载均衡软件）＋Haproxy（7层负载均衡软件），调度到不同的squid服务器（缓存blob图片），squid访问源服务器
#### 淘宝CDN特点
分级存储：采用SSD＋SAS＋SATA应对不同热度的数据，如今SSD便宜，全部SSD即可
低功耗服务器的定制
#### 其他
一致性问题，需要比较实时的推送更新，不能只等自动超时，热门内容可能一直驻在CDN中

## 分布式健值系统
