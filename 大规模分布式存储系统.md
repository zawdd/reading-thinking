## 概述
分布式存储系统的特点：**可扩展，低成本，高性能，易用**
主要技术：**数据分布，一致性，容错，负载均衡，事务与并发控制，易用性设计，压缩／解压缩**
1. 分布式文件系统
存储数据类型：**Blob（Binary Large Object），定长块，大文件**
2. 分布式健值系统
3. 分布式表格系统
4. 分布式数据库

## 单机存储
Hash表，B树在机械磁盘，SSD的实现就是单机存储引擎。
NUMA架构，每个节点是一个SMP结构，以网络链接
为避免网络拓扑结构的依赖，Google采用扁平拓扑结构，三级CLOS网络
先知道硬件的性能指标和价格，才能知道瓶颈，要用什么技术，解决那里的问题，比如硬盘的随机读写慢问题
SSD有写入放大问题（[Write amplification](https://zh.wikipedia.org/wiki/写入放大))，会影响其实际写入带宽

### Hash存储引擎
1. Bitcask 仅支持追加操作，文件写到固定大小后开新的文件，同一时刻只有一个新的文件，hash索引每个key对应的文件id，长度，偏移，以便于找到对应的value，对于删除，只修改value值为特殊值，不真的删除
2. 定期合并删除和被更新后的垃圾数据，生成新文件
3. 快速恢复，防止内存中的hash索引丢失，每次合并删除时，创建索引文件，相当于把内存中的索引dump到硬盘，恢复时直接读文件

### B树存储引擎
关系数据库以此为底层存储实现，支持随机存储和范围扫描，如InnoDb
数据库的缓存算法LIRS，把缓存分为两级，热点数据进入第二级，第一季采用LRU替换，防止全表扫描等操作使得缓存池中的大量页面被换出

### LSM树存储引擎
log Structured Merge Tree，把数据的修改增量缓存在内存中，批量操作磁盘。读取时合并磁盘和内存数据。
LevelDB，数据写入时，先写日志，然后写内存，内存满了，开一块新的继续写，写的操作不能被hold住。硬盘存的文件有顺序性，和一些索引信息，使得内存数据写入硬盘时性能好

### 数据模型
* 文件模型，对象模式弱化的文件模型，POSIX标准
* 关系模型，表格，SQL
* 键值模型，NoSql，put get delete

传统SQL数据库在海量数据场景面临的挑战：
1. 事务，ACID特性
2. 联表，违背第三范式，A表中存B表中的非主键信息
3. 性能

### 事务与并发控制
COW(Copy on Write) & MVCC(Muti-Version Concurrency Control)
可串行化：多个事务并发执行的结果，和按照某个顺序串行执行的结果相同
MVCC:以InnoDB为例，每一行存一个被修改的“时间”和删除的“时间”，这里的时间就是数据库的版本号，每次执行一个事务时，会在分配一个版本号给该事务，根据此事务号和每行的版本号比较，判断某一行是否该参与此事务。Update时，会先copy一份
故障恢复时纪录的日志，UNDO／REDO 顾名思义，简化日志的记法，批量刷日志到磁盘。定期dump数据，checkpoint技术。REDO日志纪录的都是修改后的结果，使得回放操作具有幂等性

### 压缩
LZ算法，后续的压缩都在此体系下，根据效率和压缩比例改进调整，以列存储实现OLAP数据库，提高压缩比例，减少查询时间

## 分布式系统
木桶原理 －> 发现短板 －> 性能估算
Paxos协议&两段提交协议
分布式系统的三态：成功，失败，超时，对于超时，要么操作幂等反复重试，要么获取刚才的操作状态，以此判断
越是强的一致性模型，使用越简单

### 性能分析
排序性能分析方法：排序时间＝比较时间（分支预测错误，5ns左右）＋ 内存访问时间（4GB/s 内存访问速度）
1GB 4字节整数快排约28s

### 数据分布
#### Hash分布
* 对于大用户处理方式：
1. 手动拆分，定时跑任务，配置拆分规则
2. 自动拆分，由数据分布算法动态调整
* 机器变化时Hash的N值变化，数据要重新部署，解决：
1. 把hash和机器的映射元信息由专门的服务器管理而不是简单的模运算
2. 采用一致性Hash distributed hash table，给每个节点分配随机token，这些token构成hash环。先计算key的hash，然后放到顺时针第一个大于或等于此hash值的token所在的节点。有点，节点删除或增加时，只影响hash环中相邻节点。但还要考虑迁移执行时负载不均衡。每台节点都纪录所有的hash环的信息。
#### 顺序分布
需要考虑顺序数据表的分裂和合并

### 负载均衡
是个30-60分钟的慢过程，新节点加入时，master不可大量迁移数据到新机器

### 复制
强同步复制，异步复制
同步常用的做法是同步操作日志，副本上回放日志
除了主副本＋复制协议，还有写多个节点的复制协议NWR，不分主副，一共N个，W个写副本，R个读副本，保证W＋R>N
CAP理论
Oracle的最大可用模式：正常时强同步复制，主备网络故障时，切换为异步复制

### 容错
租约Lease协议，原因，异步网络中的多台机器无法达成一致。
A机器给B机器发送带有时间的租约，时间会有时间服务器同步

### 扩展性
水平扩展除了考虑master节点的瓶颈，也需要考虑故障恢复和自动化扩容的灵活性
master瓶颈时，增加一层，来存元数据，由于客户机会缓存中间的元数据服务器的信息，所以不会增加额外网络消耗
扩容，双倍扩容的劣势
应采用异构系统，而非同构系统，同构系统需要其中一个分片copy扩容，慢，异构时数据分散在所有系统，由全系统来copy数据扩容

### 分机房
多个副本构成一个复制组，靠Paxos协议保证只有一个主副本，不需要master节点发放租约，master故障不影响worker，工程复杂度高

## 分布式文件系统
一方面存储Blob文件，一方面是分布式数据库的底层存储
### GFS
分为master，chunkserver和client
1. 租约机制
为了防止所有的写操作都通过master，通过lease给某一个chunkserver，授权为主chunkserver，可以执行写操作。每个chunk维护版本号，解决不一直问题，每次发租约副本的版本号加1，版本号低的chunk进行定是垃圾回收
2. 一致性模型
GFS is designed for append not for overwrite, append is simple。需要解决追加执行多次，产生重复。多个客户端追加的顺序性，GFS不保证一个client的多次追加是连续的
3. 追加流程
请求master获取所有chunk元信息，数据client发给每一个副本，每个chunkserver内以LRU结构缓存数据，所有的副本都确认受到数据后，client发起写请求给主副本，主副本确定对同一个chunk多次append的顺序并写入，主副本把写请求发给其他副本，其他副本根据此顺序写，其他副本写完后回复主副本，主副本再应答client，如果部分从副本不成功，则client重试。
**利用流水线操作，减少延时**，当一个chunkserver收到一些数据，就立刻转发。采用全双工网络，发送不降低接受的速率。
**控制流和数据流进行分离**，利用网络拓扑更好的传递数据，每个机器把数据，发给拓扑结构上最近的另一台机器上。前提是每次追加的数据都比较大。
追加流程还需要考虑，过程中，主副本租约过期，失去chunk修改操作的授权，主副本和备副本所在的chunkserver出现故障等各种异常。
*可以从人角度考虑，租约过期就给予即两次续约申请的机会，在这个阶段hold资源，若成功则继续，否则算作失败，走失败逻辑，对于各种可能的状况，都去一一设计对应的逻辑解决可能复杂度太大，可以统一都回退到一个失败状态，然后再去处理*

4. 容错机制
master:操作日志＋checkpoint＋实时热备
chunkserver:复制多个副本容错，全部副本写入成功才认为成功，64mb的chunk中分为64kb的Block，每个block中存32位校验和。

#### Master设计
1. 需要估算维护元数据的内存占用，结论1pb数据不会超过3gb内存不会是瓶颈，采用写时复制的B树，满足元数据管理，实现复杂
2. 负载均衡，考虑网络拓扑，机器的分布，磁盘的利用率。新副本所在的chunkserver的磁盘利用率低于平均，但是要限制每个chunkserver“最近”创建的数量，防止新加一个chunkserver时瞬间的大量迁移把他压垮，所有的副本不能再同一个机架。
当chunk数目少的时候需要复制，复制的任务也有优先级，副本最少的chunk先复制。只有一个副本的chunk要禁止其写入。
3. 垃圾回收，延迟删除，先标记，后删除。chunkserver和master的心跳消息中同步master中已经不存在的chunk消息，然后chunkserver去释放副本，在使用的低峰时段做这件事。
4. 快照，写时复制生成快照，chunk利用增加引用技术的形式，master生成新的快照文件后，还指向原来的chunk。真正有写入时，再copy这个chunk，写入到新的chunk中

#### ChunkServer设计
chunk要尽量均匀分布再不同的磁盘，还要考虑磁盘空间，最近新建chunk数量，需要删除的chunk只是移动到磁盘的回收站，下次重用，作为磁盘和网络IO密集型应用，做磁盘和网络操作的异步化，最大化的发挥性能。

### Taobao File system
数据量巨大，Metadata单机无法存储
减少图片读取的IO次数，普通linux读一个文件要3此磁盘IO，读目录元数据，把文件inode节点转入内存，最后读文件
多个逻辑图片文件共享一个物理文件

#### 架构
不维护文件目录树，每个文件用64位编号，*退化为了KV的存储了就*，将大量小文件合并成一个大的块，以block id＋offset来确定文件，三备份存储，客户端不缓存文件数据
1. 追加流程
由于TFS读多于写，所以写流程简化，所有的写操作都经过master，不需要支持多个client并发追加，同一时刻每个block只能有一个写操作，多个client的写会被串行化。client从master找可写的block，客户端先写主副本，主副本再同步数据到备份副本，成功后master需要根据主副本的反馈更新block的版本号，然后再返回client成功，其中的任何失败都从第一个步开始重试
2. 其他问题
文件重复，根据文件的指纹，建立去重系统，这个网盘也是类似，去重本质上又是一个KV的存储
文件更新，写入新文件，再应用系统中保存新文件的位置
文件删除，应用系统中删除，TFS中本身不删除

### Facebook Haystack
估算每天的量时，一台计算位40000s，而没有算作86400s。
#### 架构
多个逻辑文件，共享一个物理文件
包括：目录(master)，存储，缓存。缓存用于解决对CDN供应商过于依赖的问题。
1. 写流程
通过目录获取可写的逻辑卷轴，生成id，备份3份，照片再每个物理卷轴的offset可能不同。只支持追加，以offset更大的照片为同一个逻辑卷轴中的最近照片
2. 容错
存储节点出错，将物理卷轴对应的逻辑卷轴标记为只读，不可恢复时将执行拷贝人数
目录容错，主备数据库做持久化，由数据库提供容错机制
3. 目录
根据图片id映射到对应的逻辑卷轴和其位置，估计使用的memcache或mysql sharding，因为映射的关系太大。也需要提供负载均衡
4. 存储
物理卷轴对应一个文件，存的图片除了二进制本身，还有元信息（构成一个needle），每个物理卷轴还维护一个索引文件，便于查找。先写文件，再异步更新索引文件，追加式的，更新也方便。
删除时，延迟删除，追加一个带有删除标记的needle，定时任务回收删除空间，扫描全部文件，只保留最新的，重新组织文件。
#### 其他
可以进行删除，TFS不能，TFS只存了blcokid＋offset，一旦删除了其他的，这个值就要该了，而应用系统中已经存好了，不可能改。而haystack中元信息只能定位到逻辑卷轴，需要在逻辑卷轴中再次定位才能找到文件，相当于多了一层，就可以删了。

### 内容分发网络CDN
CDN本身由两层cache，图片服务器本身还有cache，都不中才会访问分布式存储
负载均衡：LVS（4层负载均衡软件）＋Haproxy（7层负载均衡软件），调度到不同的squid服务器（缓存blob图片），squid访问源服务器
#### 淘宝CDN特点
分级存储：采用SSD＋SAS＋SATA应对不同热度的数据，如今SSD便宜，全部SSD即可
低功耗服务器的定制
#### 其他
一致性问题，需要比较实时的推送更新，不能只等自动超时，热门内容可能一直驻在CDN中

## 分布式健值系统
比较适合哈西分布算法
### Amazon Dynamo
应用了很多分布式技术，牺牲了一致性，却没有得到什么好处。组合了P2P的各种技术
1. 数据分布
采用基于随机token的一致性hash算法，为了适应不同的节点，处理能力差别可能很大，进行改进，根据性能一个节点可以分配多个token，一个token对应一个虚拟节点，每个节点维护整个集群的信息，用于定位。为了解决节点的加入和删除，采用Gossip协议每1s选择一个节点，为自己的通信节点
`Gossip 协议用语P2P系统中自治的节点协调对整个集群的认识，两个节点，交流自己保存的全集群信息，互相更新自己的信息，新节点通过种子节点第一次交换信息，hash环(DHT)中的其他的节点也定期和种子节点交换信息，发现新节点`
2. 一致性与复制
当节点失效时，复制数据，数据存N份，DHT定位到节点K，则存K，K＋1。。。K＋N－1这些节点上，K＋i失效时，找K＋N节点替代，如果K＋i重启，则K＋N通过Gossip协议发现，将数据回传（Hited Handoff)。如果K＋i确认宕机，则利用 **Merkle树** 对机器的数据文件进行快速同步。采用NWR，保证故障时依然可读。但是P2P集群，每个节点存储的集群信息不同，可能同一个纪录被多个几点同时更新，无法保证节点的更新顺序，为此又引入 **向量钟（Vector Clock)** 技术解决冲突。
`以［nodes，counter］表示向量钟，nodes表示节点，counter计数，每次节点更新则＋1，每个节点处理一个对象，就把（s1，1）这样的数据加入向量中，一个对象同时被s2和s3修改，就会[(s1,1)(s2,1)],[(s1,1)(s3,1)]两个冲突的，可以让应用程序解决冲突，也可以根据时间戳，用最新的，但是时间的同步不能精确准确`
Dynamo只能保证最终一致性，clinet有可能读到不是期望的结果。
3. 容错
异常分为临时和永久的
* 数据回传
临时实效的节点又上线后，把替代节点上接受的数据回传给原节点
* Merkel树同步
原理：每个非叶子节点对应多个文件，为其所有子节点值组合以后的hash值，叶子节点对应单个数据文件，为文件内容的hash，每台机器对每一段范围的数据为何一颗merkle树，这样有不同从根节点到叶子都能体现出来，要同步时同步不一样的即可。
* 读取修复
当一个节点写入返回客户端说成功，但是立即宕机，还没有同步给其他机器，这时候读的不同副本结果可能不同，需要异步定时修复任务。
4. 负载均衡
应考虑新加入和删除节点对集群的影响，包括merkle树等各种数据的影响，都需要重新算。完全随机分配token就是这样。
数据范围等分＋随机分配，相当于加了一层，避免节点变动需要重新计算全局信息，哈西空间分为机器数X虚拟节点数。
由于Dynamo中后台任务很多，维护一个资源授权系统，将整个机器的资源切分成多篇，监控60s内的磁盘响应时间，事务超时，锁冲突情况，同台调整分配给后台任务的资源片数，防止影响系统的读性能。
5. 读写流程
找到所有需要写入的节点后，选一个为协调者，某一个副本写入成功后都返回给协调者，失败重试，有W个写成功后，返回client成功。
读取时，也从存数据的副本中选一个协调者，根据负载策略选R个读副本，并发读。当R个读成功后，如果R个完全一致，则直接返回。否则，根据处理冲突的规则合并冲突。默认以时间戳。成功应答Client后不会立即释放，再等一段时间，可能还会有其他得节点数据返回过期数据，协调者需要做一个数据修复。
6. 单机实现
可插拔的存储引擎，Berkeyly DB，Mysql InnoDB适应不同的存储对象。基于时间驱动，利用状态机。
7. 其他
一致性得问题，导致异常状态下得测试十分困难。

### 淘宝Tair
可以有持久化和非持久化两种使用方式。
1. 架构
config server＋data server M主备，S等价
2. 数据分布
计算hash后，分布到Q个桶中，桶是负载均衡和迁移得基本单位。master把每个桶指派到不同的data server上，根据Dynamo论文的实验结果，Q需要取远大于集群机器的数目，如10240。*用不用素数？*
3. 容错
对于备副本故障，复制一个新的，对于主副本，则先提升一个备为主。
4. 数据迁移
负载不均衡，加入新机器时，桶需要迁移，迁移时需要保证服务可用，A迁移到B，迁移完成的，转发到B，还在迁移的依然由A提供，有修改的请求，A上改，改完，再把操作日志传给B，A和B完全一致后才算迁移完成。
5. Config Server
客户端缓存路由表，一般不访问config，每次路由变更，config推送信息给data server。路由信息有版本号，客户端访问时，会把自己缓存的发给data server，比较版本号。若过期，需要访问config再获取。如果访问不到data server则直接取访问config获取新的路由表。
6. data server
抽象的存储引擎层，方面更换存储引擎。插件容器，动态加载卸载插件。

## 分布式表格系统
每个表格有多行组成，通过主键唯一标识。
### Google Bigtable
基于GFS和Chubby的分布式表格系统，存储结构化和半结构化数据。行和列构成单元，单元还包涵多个版本的数据。将多个列组织成列族，列名由两部分组成（column family, qualifier），作为访问控制的基本单元，bigtable创建的时候可以有几个family，每个family的qualifier可以任意多个，因此适合标识非结构化数据。
行主键是字符串，字典排序，对于网址，倒序存储便于同一个网站下面的所有网址连续存储，每个单元的版本可以只保留最新的N个，也可以保留制定时间段的。
1. 架构
bigtable本质在GFS上增加了分布式索引层，加上chubby（分布式锁服务）进行选举和全局信息维护，结局了GFS遗留的一致性问题。bigtable把大表拆成100-200mb的tablet子表，则架构有client，master和tablet server构成。子表需要分裂和合并。
chubby用来保证一个master，存储bigtable系统引导信息， 发现子表服务器的加入和删除，获取schema信息，访问控制信息，核心算法是Paxos。
用户表，元数据表（存用户表的元数据），根表（存元数据表的元数据），chubby存跟表的元数据。
2. 数据分布
通过两层的元数据表，提升支持的数据量，但每次修改都要改两层元数据。采用缓存和预取（每次查子表元数据时，一次多获取一个连续的字表元信息）
3. 复制和一致性
强一致性，通过chubby的互斥锁保证同一时刻只有一个tablet服务器处理一个子表，GFS本身是弱一致性，只实现了atleast once，bigtable会写操作日志到GFS，日志有唯一编号，避免了GFS的纪录重复。每个子表中的SSTable数据，虽然会写入多次，但是只会索引最后一条。
4. 容错
tablet在chubby中获取独占锁，master定期探查，如果无响应，则除了tablet出问题，还可能是chubby出问题，master会现自己探测次排除chubby的问题，tablet出问题，则迁移其数据（包括日志和SSTable),对于故障tablet server上的还未执行的日志，由于改日志是混合着全部存入GFS的，所以Master选负载低的机器，对日志排序，选择要恢复的tablet的连续的日志，然后让恢复数据的sever执行。
5. 负载均衡
tablet server向master汇报负载，高负载的子表先从tablet server上卸载下，再找一个负载低的加载。过程中将内存中的跟新操作以SSTable文件形式存在GFS，避免负载均衡迁移导致子表在新的tablet server上需要重放操作日志。这个叫做（Minor Compaction），过程中会停止服务
6. 分裂与合并
子表分为内存中的MemTable和GFS中的SSTable，分裂时只分索引。分裂后先写不同的内存Memtable，执行Compaction操作时再生成不同的SStable。事务保证只要元数据修改成功，则分裂操作成功。合并时，需要迁移使得不同子表在一个tablet server，然后合并。非常麻烦。*个人觉得：迁移＋合并时都要停止服务，还要考虑过程中各种可能的失败*
7. 单机存储
先写操作日志，然后是内存memtable，一定量后dump到磁盘sstable。多个sstable可以合并，内存中的数据总是新的，所以读取时新旧还需要合并。采用Merge－dump引擎，把插入，删除，更新，增加看作一回事，sstable只纪录操作而不是最终结果(*所以需要强一致性*)。
数据再SStable中按照主键顺序存储，每个sstable由若干大小相近的数据块，tablet server有块缓存和行缓存。随机读取时顺序是，行缓存－快缓存。支持 **布隆过滤器（Bloom Filter)**，即如果行再SStable中不存在，则不用读GFS文件就能发现。
8. 垃圾回收
master定期执行垃圾回收任务，标记删除过程，扫描GFS所有的SStable，如果改子表的再master的元信息里没有，则回收。由于生成新的sstable和修改元数据不是原子的，所以垃圾回收，一般只回收至少一段时间以上的数据。
9. 讨论
单副本服务，故障恢复时无法提供服务，不适合实时性特别高的。SSD便宜，作为存储和服务分离的架构有些不适应。架构的复杂性，导致bug难以定位。

### Google Megastore
在Bigtable的基础上提供友好的数据库功能，介入关系数据库和Nosql之间。同一个用户要求强一致性，但多个用户之间只需要最终一致性。一个用户的所有数据构成一个实体组Entity，每个entity表中的一行，保证操作原子性，同一实体组数据连续存放，即一个用户的数据在一起。实体组由许多表构成，有根表和子表，根表的行为根实体，还保存有元信息。
1. 架构
客户端库，把megastore操作映射到bigtable，事务控制等。复制服务器，转发请求到所在机房bigtable实例，解决跨机房连接过多，协调者，快速读，存储每个机房本地的实体组是否处于最新的状态的信息。
2. 实体组
实体组内日志以Paxos方式同步到多机房，保证一致性。实体组之间以分布式队列保证最终一致性和两段提交的事务。实体组以REDO日志方式实现事务。
3. 并发控制
读取分类，最新读取（事务写操作都完成），快照读取（读取最后一次事务执行完后的），不一致读取（直接读，可能会读到没完成的事务）
写事务，预写日志，一个写事务总开始于一个 最新读取，以paxos复制协议的乐观锁保证写日志时，如果有多个写操作，只会有一个成功，其他失败后重试。由于同一个实体组（一般都是一个用户）同时进行的更新往往很少，事务冲突导致的重试概率会低。
4. 复制
Paxos的复制协议主要用来解决，master宕机时，在确认master宕机这一事件时，需要停止写服务的问题。paxos允许在只是怀疑master宕机时就可以以投票的形式保证多个节点同时修改时，只有一个成功。
5. 索引
实体组内有局部索引，还有全局索引，索引中可以直接额外存储数据，这样就不用查表了。
6. 协调者
由于paxos要保证至少读一般以上的节点，才能保证最新，这就慢。所以利用协调者，记录本地机房的状态，如果本地已经是最新的，则直接读本地就行。以chubby锁来防止协调者宕机。生效操作要严格满足一些列竞争条件。
7. 读取流程
本地读和多数派读，后者读后还需要使本地的数据追赶变为最新的
8. 写入流程
以Paxos协议协商一个可写日志的提议号，都同意后写日志，不同意的少数，给其协调者标记失效。这里只说写日志，是认为日志正确的写之后，数据可以REDO日志。*本地REDO日志一定正确么？*
9. 讨论
分布式系统的两个目标：线性扩展，支持全功能SQL。
以实体组的形式划分数据，还避免了Join。
架构复杂，实时性查，协调者对于读写和运维的复杂性。

### Windows Azure Storage（AWS）
包括Winfows Azure Blob/Table/Queue
1. 架构
定位服务＋存储区，底层用结构控制器，管理硬件资源。存储区又分了三层，文件流层，类似GFS，分区层，类似Bigtable，将对象分到不同的分区，前端层，无状态Web服务器，路由，验证，缓存等功能。以DNS把每个账户的请求定位到所属分区。
存储区内复制，强同步。跨存储区复制，异步复制，容灾。
2. 文件流层
写操作追加，一个文件流，包涵多个extent，每个里又是多个连续block，block为读写最小单位4mb，extent三副本，负载均衡基本单位。
以Paxos协议实现高可用，对元数据和日志流加唯一事务编号，防止重试导致的重复记录，对于数据，只又最后一个追加的内容被索引。AWS写时主副本不变，所以不需要租约机制，简化追加流程。
磁盘的大块顺序读写会阻塞随机读写，要考虑这个公平性。在磁盘中加一个顺序的日志盘，减少随机写。**抹除码(erasure coding)** 减少extent副本占用空间，对已经缝合的extent进行 **Reed-Solomon编码**，一个extent分成N份，编M个码，损坏少于等于M个就能恢复。
3. 分区层
强一致性和事务的顺序性。
4. 讨论
相比于GFS＋Bigtable，提升extent大小到1g，减少元信息，从而只需要一级元信息，每个extent完全一致

## 分布式数据库
思路：应用层划分数据，mysql sharding，关系数据库内部自动分片 microsoft sql zure， 从存储引擎开始重新分布式数据库
### 数据库中间层
以mysql sharding为例
1. dbproxy集群，执行SQL路由，过滤，读写分离，结果归并，排序，分组。无状态，不存在单点问题。client和dbproxy中间层中可以增加linux virtual server（LVS）做负载均衡，为了减少LVS的延迟，客户端可以直接配置中间层服务器列表。
2. 数据库分多组，每组内有主备，通过binlog同步，主机负责写事务和读一致性。
3. 元数据服务器，维护dbgroup拆分规则，确定SQL执行计划，本身也需要多副本，常用zookeeper实现。
4. 常驻进程agents，部署在每台数据库服务器上，监控性能，主备切换等。
一般一个用户的请求发到一个数据库组里，也需要考虑单个用户数据过大。需要多个分组时就需要在dbproxy中合并
#### 扩容
1. 集群容量不够怎么办，停机，双倍扩容
2. 单个用户数据量太大，应用层定期统计大用户，把大用户数据拆分到过个dbgroup，这里维护这些对应用层来说时个很大的代价。*能否解决？再引入一个第三方？*
3. 难以自动化，代价大，有的查询需要跨所有的分区

### Microsoft SQL Azure
1. 数据模型
构建在SQL Server上，将数据划分为多个分区，限制事务只能在一个分区执行，规避分布式事务。表格组，有主键，其中所有表格有小相同的列，以此划分。
2. 架构
分区有多副本，主备不能在同一个故障域。
3. 复制一致性
采用Quorum Commit复制协议，保证一致性，主副本执行，把消息发给备副本，主回滚，备删除事务，主成功，备再执行，超过一半正确，再返回给客户端。
消息日志，可能错误，需要做校验，磁盘可能会出现“位翻转”也需要写入时校验。
4. 容错
有全局分区管理调度，主坏，选一个备当主，备坏，选低负载的复制。调度器本身存的元数据有7个副本进行容错。
5. 负载均衡
副本迁移，主备副本切换，新节点加入，任务执行的频率要调度，均衡负载，当主副本负载过高，可以切换主备，用备当主，不用拷贝数据。负载包括：读写次数，磁盘，内存，cpu，io使用量。
6. 多租户
限制每个用户的操作相互干扰，如限制操作系统资源，限制数据库的容量。

### Google Spanner
全球级分布式数据库，数百数据中心，数百万台机器，通过同步复制和多版本控制满足外部一致性，支持跨数据中心事务。
#### 数据模型
层次化，最底层是目录表(directory table)，其他表创建时制定层次关系，相当于megastore中的实体组，比如一个用户的信息和这个用户下所有的照片信息，构成一个目录，一个目录的数据会存在一起，每个副本分配到同一台机器，针对这个目录的事务大多数不跨机器操作 *但是一台机器不是属于一个故障域么？*
#### 架构
Spanner构建在google下一代分布式文件系统Colossus上，后者比GFS改进了实时性，支持海量小文件。
由于全球性，有新概念，
Universe：一个Spanner部署的实例，全世界一共3个，开发，测试，线上，支持多数据中心，多个业务一个universe。
Zones：每个Zone属于一个数据中心，Zone内部通信代价低，之间高。
Universe Master，监控universe里Zone级别的状态
Placement Driver：跨zone数据迁移
Location Proxy，获取数据的位置服务，client通过其知道数据在那个spanserver。
Spanserver：提供存储服务，每个服务多个子表，每个子表包涵多个目录。
怎样处理目录和Spanserver的映射关系？猜测把此元数据，做着表格，也存在spanner中。
#### 复制和一致性
每个数据中心运行一套colossus，一个机器上有100-1000子表，子表在多个数据中心多副本。每个子表运行一个Paxos状态机，为了同步系统中的操作日志，Paxos选一个副本为主副本，寿命默认10s，正常时会结束时继续选为主，异常时，其他的副本10s后选为主 *这之间执行的事务怎么办？* ，每个主副本所在spanserver实现锁表用于并发控制，读写操作某个子表的目录时，以此避免相互干扰。还有事务管理器，若事务在一个paxos组里，则逃过管理器，否则需要事务管理器协调。实现跨多个paxos组的分布式事务，还需要实现两段提交协议，一个paxos组的主副本为两段提交的协调者，其他为参与者。
#### TrueTime
给事务分配全局唯一事务id，很难，专门部署一套oracle数据库用于生成此id。
Spanner采用全球时钟同步机制truetime，返回本地事件的接口，除了返回时间戳t，还返回误差e。实现基础是GPS和原子钟。数据中心部署一些主时钟服务器，其他从服务器从其同步时钟，有的主时钟用GPS，有的用原子钟。每30s和若干主时钟服务器同步，排除偏差比较大的结果。
#### 并发控制
使用TrueTime并发控制。支持，读写事务，只读事务，快照读事务，客户端提供时间戳或范围。
事务需要用时间戳标示提交的版本，事务执行时需要比较时间戳来返回数据，利用两段提交保证一致性，也用时间戳。
利用truetime的返回事件比较俩事务的提交时间，采用延迟提交手段，保证只要事务t1的提交操作早于t2的开始操作，则t1的提交版本小于t2的提交版本，每个事务延迟2e，从而严格保证t1，t2的顺序。
#### 数据迁移
目录是基本单位，一个paxos组包涵多个目录，paxos组可以切分，先移动数据，然后再原子操作更新元数据。
#### 讨论
底层分布式技术实现可扩展性，上层关系数据库模型和接口。

## OceanBase
数据分为基线数据和增量数据，基线数据只读，定期把增量融合到基线中。
一个业务例子，收藏夹应用的根据价格排序，传统的分库分表不堪重负。
跨行支持事务，bigtable只支持单行事务。最简单的，在bigtable的开源实现（HBase或Hypertable）上增加两段提交。
分析业务特点，虽然数据量大，然是最近一段时间的修改量往往不大，总量几十上百亿，一台修改几千万几亿 *凡事还应从实际业务出发*，以单台更新服务器记录修改，基线数据以分布式存储，每次查询合并基线和增量数据 *lambda架构？*，事务都落在一台机器上，避免复杂的分布式事务，单机以硬件升级的形式扩展。
以前的数据库，都是基于磁盘优化设计的，现在SSD大量普及。

### 架构
1. rootserver：管理，同一时刻只允许一个updateserver提供写服务，牺牲可用性，满足强一致性。采用租约机制。采用主备，主备间数据强同步，采用[linux HA（心跳）](www.linux-ha.org) 实现高可用性，共享VIP，故障发生时VIP（虚拟IP）自动漂移。
2. updateserver：更新增量数据，为了性能数据都存在内存中，过多后再快照到本地SSD，增量合并到基线时，冻结内存先。之后写入新的内存中。然后告诉root心跳数据版本变化，再通知chunk更新，采用多路归并和并数据。
3. chunkServer，存储基线数据，类似bigtable，分子表，根据主键顺序分布，
4. mergerserver；接受用户请求，合并chunkserver和updateserver的数据。支持SQL协议，直接和client通信。 *merge的数据量如果太大怎么办？利用磁盘做外部排序？* 还需要执行jion，嵌套查询等。对于读写操作，merger把基准数据还要再传给update。
客户端实现具体选择访问那一台mergerserver的逻辑，先访问一个数据库，获取他们的地址，然后选择。一致性hash，便于将同一个sql发到同一台mergeserver便于缓存。通过不同集群间的流量的参数配置，决定请求发到不同的集群，也便于上线新功能时主备替换。
主备同步，提供强一致性，要求主备再一个机房，或者代价下，利用异步主备，容灾。
针对update这一个单点，性能分析，网络专门优化。由于记录都要先写日志，会对磁盘有瓶颈，给磁盘配备带缓存的RAID卡，其带电池，保证缓存可以刷入磁盘。多个写事务合并提交，使得多个用户的操作凑成一批，较少磁盘IO。
5. SSD支持，SSD随机写不行，以块为单位，及时只写一个字节，也要擦出整个块，再写，写放大效应。系统设计时，摒弃随机写。
6. 数据正确性，tcp协议，磁盘，程序bug都会导致数据损坏。采用数据存储校验CRC，传输校验CRC，主备update数据镜像校验，对整个内存中的表计算校验码，要求主备一致。数据副本校验，chunk合并生成新的子表时，生成校验码汇报给root。

### OceanBase的分布式存储
#### 公共模块
1. 内存管理
Google TCMalloc管理库，通用不如专用。初期内存管理不在高效，而是可控性，防止碎片。对于64kb内存，系统自己用一个链表管理，大于64kb直接malloc申请。每个线程缓存若干内存块。全局内存池便于统计每个模块内存使用，发现泄漏。可以辅助调试，给内存块赋值0xFE等非法值，内存越界时服务器快速出现core，不会运行一段时间后再core。
2. 数据结构
哈希表：加入位锁，发现冲突。延迟初始化，所有的hash桶一共1000w个，分为若干单元，每个6w个，这样如果这个单元不存在，则再初始化。
B树：索引结构，支持多线程并发修改，写时复制，读取不阻塞。不支持删除，通过追加删除标记。 *这种系统不适合网盘这种，存储比较稀缺的应用，存数据时存储可以浪费些*
3. 锁
共享锁，互斥锁，保证行的事务
4. 任务队列
全局一个队列，锁竞争重，可以每一个工作线程一个队列，按照一定策略增添任务。为了解决每个线程不均衡，一个工作线程自己没有任务后，可以遍历其他所有线程的队列。对于全局队列，让不同的线程等在队列的不同槽位上，避免锁冲突。
5. 网络架构
接受到网络包后，放入全局队列，通知工作线程处理。虽然客户端有异步和同步请求，但其实server端都一样的。
6. 压缩和解压缩
插件化，第一次调用压缩或解压缩的方法时，才动态加载动态库。LZO，Snappy

#### RootServer 实现
采用写时复制，由于修改特别少，采用有序数组，而非B＋树或跳跃表（Skip List），批量写。不够优雅。
以租约实现updateserver选主，保证只有一个update提供服务。
设计了优雅退出机制，当root更新时，需要下线，给update发超长租约。

#### UpdateServer 实现
采用Direct IO写操作日志，防止污染操作系统缓存。日志也有校验和。
行操作链表，以每行的key为开头，存改key对应行的每个单元的修改的链表，key作为B树索引结构的叶子。采用整数的变长编码防止内存膨胀
update缓存预热，冻结内存前，把一定比例的请求给SSD上的sstable，这样丢弃memtable时，切过去后自然就预热了。
频繁的小数据包，如果多个工作线程，从一个队列获取，需要加锁，上下文切换频繁，会达到瓶颈。改为多个线程收发包，每个线程收到网络包后即刻处理，较少上下文切换。任务分为长的和短的，对于长的还需要加锁，处理。每个线程对应一个网络套接字，又会出现不均衡的问题。

#### ChunkServer 实现
缓存：hash＋lru，hash用于查找，利用访问次数的和访问时间的排序，避免LRU移动链表时锁住，牺牲了一定的精确性
惊群效应，如果一个热门行，N个线程都读这一行，会加N次锁，为了避免，第一个线程发现该行不在时，先设一个标记，其他线程会先等会儿。
1. IO实现，没用采用系统本身的页面缓存机制，自己实现，采用Direct IO，支持磁盘IO和CPU并行，采用linux的Libaio实现异步IO，以及 当前 和预读 双缓存。当前 和 预读 这两者来回交替，用来装异步读到的数据，给上层CPU执行。双缓存广泛应用于生产者消费者模型。
对于需要大量导入数据的场景，不通过update，直接mapreduce排序，生成sstable，给chunk然后做子表合并。
业务场景是有天花板的，单个节点如果性能足够强也是可以应对的 *未来是否会不需要分布式？*

### 数据库功能
支持的SQL比较简单，优化器不行。但是针对业务有其他的优化。
#### 结构
词法分析：GNU Flex，语法分析：GNU bison，生成逻辑执行计划和物理执行计划。逻辑计划是执行的意图，物理计划才是具体执行的方法，这里就设计到了优化。
* 排序
支持了内存不够时的外排序。多路归并。
* Hash算法
hash用来做groupby 和 disctinct，难点在于数据总量超过内存上限的处理，*可以分块一次处理一部分，利用磁盘*
* Join
基于排序算法，和基于Hash的jion。
#### 事务
行锁，死锁时超时回滚。
#### OLAP业务支持
每次SQL查询的数据量大，只读部分列。解决：并发查询＋列存储
1. 并发查询：子请求失败，会发给别的chunkserver再次执行，SQL本地化原则，尽量在chunk上执行完，削减数据量。mergerServer对于大数据量是瓶颈。需要解决 *估计还是分治*
2. 列式存储：列组（经常一起访问的列），要设计好内存数据结构，不然CPU会是瓶颈
#### 大表左连接
基线数据中，左表冗余右表中需要的信息。增量数据中，分开存储。利用低峰合并，增加冗余而又不太影响效率。
表增加创新时间列，修改时间列，便于自动过期。也便于错误数据的删除。

## 质量保证，运维
